{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transformers + peft (LoRA) for generation\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Logging\n",
    "# -----------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# User-editable settings (no hard-coded paths)\n",
    "# -----------------------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "BASE_MODEL_ID = \"your-model-id\"                 # e.g., \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "CKPT_PATH = \"path/to/dp_lora_checkpoint.pth\"    # saved model.state_dict() from training\n",
    "PROMPTS_PATH = \"path/to/prompts.pkl\"            # pickle dict -> values are prompt strings\n",
    "\n",
    "DEVICE_INDEX = 0                                # choose GPU index if available\n",
    "\n",
    "PROMPT_LIMIT = 1304                             # set None to use all prompts in PROMPTS_PATH\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "MAX_PROMPT_LEN = 324                            # 512, 768\n",
    "MAX_NEW_TOKENS = 700                            # 1280\n",
    "\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7                               # 0.3, 0.5\n",
    "TOP_K = 50                                      # 10, 50\n",
    "REPETITION_PENALTY = 1.2\n",
    "\n",
    "NUM_REPEATS = 3                                 # save synth outputs 1..NUM_REPEATS\n",
    "RUN_TAG = \"run_tag\"                             # used in output filenames, e.g., \"100context_1dp\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# LoRA settings (must match training)\n",
    "# -----------------------\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05                             # 0.0\n",
    "INCLUDE_LM_HEAD = True                          # False if lm_head was not LoRA-wrapped\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def sanitize_prompts(texts: List[str], eos_str: str) -> List[str]:\n",
    "    \"\"\"Remove accidental EOS markers from prompt text.\"\"\"\n",
    "    cleaned = []\n",
    "    for t in texts:\n",
    "        t = t.replace(\"</s>\", \" \")\n",
    "        if eos_str:\n",
    "            t = t.replace(eos_str, \" \")\n",
    "        cleaned.append(t.strip())\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def strip_opacus_prefix(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Opacus sometimes wraps modules and prefixes keys with '_module.'.\n",
    "    Strip that prefix if it exists.\n",
    "    \"\"\"\n",
    "    if state_dict and all(k.startswith(\"_module.\") for k in state_dict.keys()):\n",
    "        return {k[len(\"_module.\") :]: v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def load_prompts(path: str, limit: int | None) -> List[str]:\n",
    "    with open(path, \"rb\") as f:\n",
    "        prompt_dict = pickle.load(f)\n",
    "    prompts = list(prompt_dict.values())\n",
    "    return prompts if limit is None else prompts[:limit]\n",
    "\n",
    "\n",
    "def build_model_and_tokenizer():\n",
    "    # Tokenizer: left padding for decoder-only models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, padding_side=\"left\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Base model: 8-bit, pinned to one GPU (if CUDA)\n",
    "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    device_map = {\"\": DEVICE_INDEX} if torch.cuda.is_available() else None\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    base.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base.config.use_cache = True\n",
    "\n",
    "    # Rebuild LoRA modules (must match training)\n",
    "    targets = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    if INCLUDE_LM_HEAD:\n",
    "        targets.append(\"lm_head\")\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=targets,\n",
    "        bias=\"none\",\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    # Load checkpoint\n",
    "    sd = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "    sd = strip_opacus_prefix(sd)\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    logger.info(\"[load_state_dict] Missing: %d | Unexpected: %d\", len(missing), len(unexpected))\n",
    "\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_batch(model, tokenizer, batch_prompts: List[str]) -> List[str]:\n",
    "    batch_prompts = sanitize_prompts(batch_prompts, tokenizer.eos_token or \"\")\n",
    "\n",
    "    enc = tokenizer(\n",
    "        batch_prompts,\n",
    "        padding=True,                 # left padding\n",
    "        truncation=True,\n",
    "        max_length=MAX_PROMPT_LEN,\n",
    "        add_special_tokens=False,     # prevents BOS/EOS injection into prompt\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    dev = next(model.parameters()).device\n",
    "    input_ids = enc[\"input_ids\"].to(dev, non_blocking=True)\n",
    "    attn_mask = enc[\"attention_mask\"].to(dev, non_blocking=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        gen = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "    new_tokens = gen[:, input_ids.shape[1] :]\n",
    "    return [t.strip() for t in tokenizer.batch_decode(new_tokens, skip_special_tokens=True)]\n",
    "\n",
    "\n",
    "def generate_with_retries(model, tokenizer, batch_prompts: List[str], retries: int = 2) -> List[str]:\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            return generate_batch(model, tokenizer, batch_prompts)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Generation error (attempt %d/%d): %r\", attempt + 1, retries + 1, e)\n",
    "    return [\"\"] * len(batch_prompts)\n",
    "\n",
    "\n",
    "def save_pickle(obj: Any, out_path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    logger.info(\"Saved: %s\", out_path)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    model, tokenizer = build_model_and_tokenizer()\n",
    "    logger.info(\"padding_side: %s\", tokenizer.padding_side)\n",
    "\n",
    "    prompt_list = load_prompts(PROMPTS_PATH, PROMPT_LIMIT)\n",
    "    n = len(prompt_list)\n",
    "    if n < 1:\n",
    "        raise ValueError(f\"No prompts found in {PROMPTS_PATH}\")\n",
    "\n",
    "    out_dir = os.path.dirname(CKPT_PATH) or \".\"\n",
    "\n",
    "    for rep in range(1, NUM_REPEATS + 1):\n",
    "        notes = []\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(0, n, BATCH_SIZE):\n",
    "            batch = prompt_list[i : i + BATCH_SIZE]\n",
    "            logger.info(\"%4d/%4d  elapsed=%7.2fs  batch=%d\", i, n, time.time() - start, len(batch))\n",
    "\n",
    "            out = generate_with_retries(model, tokenizer, batch, retries=2)\n",
    "            print(out)\n",
    "            notes.extend(out)\n",
    "\n",
    "        logger.info(\"Repeat %d | Total generated: %d | Total time: %.2fs\", rep, len(notes), time.time() - start)\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"synth_{RUN_TAG}_{rep}.pkl\")\n",
    "        save_pickle(notes, out_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b216d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
